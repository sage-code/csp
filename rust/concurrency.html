<!DOCTYPE html>
<html lang="en" data-bs-theme="dark">
<head>
  <meta charset="utf-8">
  <meta name="description" content="Parallel compunting, concurrency and non-blocking in Rust.">
  <meta name="author" content="Elucian Moise">
  <meta name="keywords" content="rust, syntax, parallel, concurrency, asynchronous">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Website title -->
  <title>Rust Concurrency</title>  

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
  <!-- Icon -->
  <link rel="icon" type="image/png"  href="https://sagecode.net/images/favicon.ico">  
  <!-- Prism Highlighter -->
  <link rel="stylesheet" href="../prism.css">
  <script src="../prism.js"></script>
  <!-- custom css -->
  <link rel="stylesheet" href="https://sagecode.net/sage.css">
  </head>
<body>

<div class="container">
        
<!-- header -->
<header class="row">
    <div class="col">
        <a href="https://sagecode.net"><img src="https://sagecode.net/images/sage-logo.svg" alt="Sage-Code Laboratory" height="80"></a>
    </div>  
    <div class="col  bottom-right">
        <a href="index.html#lang-index" rel="nofollow">index</a>&lt;--
    </div>
</header><hr>


<h2>Rust Concurrency</h2>

<div class="alert alert-secondary shadow-sm">
Concurrency and parallel computing are powerful techniques that programmers use to optimize the performance of their applications. Concurrency refers to the ability of an application to carry out multiple tasks simultaneously, while parallel computing refers to the ability of an application to divide a task into smaller sub-tasks and execute them simultaneously. Rust offers several tools and idioms for programmers to implement concurrency and parallel computing effectively.
</div>

<p>Concurrency is the ability of an application to perform multiple tasks simultaneously. Rust offers several tools to implement concurrency, including:</p>

<ul>
  <li>Threads: Rust provides threads that can execute concurrently within an application. Using threads, we can execute multiple tasks simultaneously.</li>
  <li>Asynchronous programming: Rust provides async/await syntax that allows for non-blocking I/O operations. With this, we can run many tasks on a single thread, which would improve performance and reduce the overhead of having multiple threads.</li>
</ul>

<h4>Example:</h4>

<p>Here is an example that demonstrates how to use threads in Rust:</p>

<pre><code class="language-rust line-numbers">
use std::thread;

fn main() {
    let handle = thread::spawn(|| {
        // function to be executed in a separate thread
        println!("Hello from a thread!");
    });

    // Wait for the thread to finish executing
    handle.join().unwrap();

    println!("Main thread exiting...");
}
</code></pre>

<p>In this example, we are using the thread::spawn function to create a new thread and execute the function inside it. We then use the join() method to wait for the thread to complete.</p>

<h2>Parallel Computing</h2>

<p>Parallel computing is the ability of an application to divide a task into smaller sub-tasks and execute them simultaneously to optimize performance. Rust provides a tools to implement parallel computing: Rayon. This is a data-parallelism library that allows for parallelization of data processing tasks easily. It can divide the data into smaller chunks and perform the computation on them independently.</p>

<h4>Example:</h4>

<p>Here is an example that demonstrates how to use Rayon for parallel computing:</p>

<pre><code class="language-rust line-numbers">
use rayon::prelude::*;

fn main() {
    let data = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10];

    // Parallel processing the data vector using Rayon
    let sum: i32 = data.par_iter().sum();

    println!("Sum of vector elements: {}", sum);
}
</code></pre>

<h4>Notes:</h4>

<p>In this example, we are using Rayon's par_iter() function to split the data into smaller chunks and sum them up independently. We can notice that this program runs faster than if we were to sum up the vector's elements sequentially.</p>

<!-- a regular diagram -->
<div class="text-center">
  <img src="../images/asynch.svg" alt="Asynchronous Execution"
       width="640" class="img-fluid protect rounded shadow border" >
  <p>Synchronous vs Asynchronous</p>
</div>


<h2>Best Practices</h2>

<ul>
  <li>Avoid sharing mutable states between threads: This avoids the risk of race conditions or unpredictable results.</li>
  <li>Prefer asynchronous programming to threads: Asynchronous programming reduces overhead by running multiple tasks on a single thread.</li>
  <li>Use thread pools: Thread pools provide a set of pre-created threads that can be reused, reducing the overhead of creating new threads.</li>
  <li>Use memory barriers: Memory barriers can ensure coherence when accessing shared data among threads.</li>
</ul>

<p>Overall, concurrency and parallel computing are powerful techniques for optimizing application performance. Rust's rich set of tools and idioms makes it easy for developers to implement these techniques effectively.</p>

<h2>Design Patterns</h2>

<p>Prallel computing is a technique used to improve the efficiency of algorithms by splitting a large problem into smaller subproblems, each of which can be executed concurrently. Rust provides several design patterns for implementing parallel computing, which can be used to take advantage of multi-core processors and other hardware to improve the performance of an application. Here are some common design patterns:</p>

<h3>Map-Reduce</h3>

<p>The Map-Reduce pattern is a technique used for parallel processing of large data sets. With Rustâ€™s Rayon library, you can use the par_iter() function to implement this pattern. The par_iter() function applies a function to each element in the iterator in parallel, and then aggregates the results. </p>

<h4>Example:</h4>

<p>Here's an example of using par_iter() with the Map-Reduce pattern:</p>

<pre><code class="language-rust line-numbers">
use rayon::iter::{IntoParallelIterator, ParallelIterator};

fn main() {
    let arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
    let sum = arr.par_iter()
        .map(|x| x + 1)
        .reduce(|| 0, |acc, x| acc + x);

    println!("Sum: {}", sum);
}</code></pre>

<h4>Notes:</h4>

<p>In this example, par_iter() applies a function that increments the value of each element in the array in parallel using all available cores, and then reduces the results by summing them up.</p>

<h3>Fork-Join: </h3>

<p>Fork-Join is another pattern used in parallel programming to split a task into subtasks that can each be computed concurrently. The crossbeam library provides several constructs for implementing fork-join in Rust, such as channels, threads, and global variables.</p>

<h4>Example:</h4>

<p>Here's an example of using channels for Fork-Join parallelism:</p>

<pre><code class="language-rust line-numbers">
use std::sync::mpsc::channel;
use std::thread;

fn main() {
    let (tx, rx) = channel();

    for i in 0..4 {
        let tx1 = tx.clone();

        thread::spawn(move || {
            let res = format!("Hello from thread {}", i);
            tx1.send(res).unwrap();
        });
    }

    for _ in 0..4 {
        println!("{}", rx.recv().unwrap());
    }
}</code></pre>

<h4>Notes:</h4>

<p>In this example, we create a channel and spawn four threads that each sends a message to the channel. The main thread waits for messages on the channel and prints them as they are received.</p>

<h3>Data Parallelism: </h3>

<p>Data parallelism is a technique that involves dividing a large dataset into smaller pieces and processing each piece concurrently. In Rust, you can use the rayon library to implement data parallelism for collections such as arrays, vectors, and slices.</p>

<h4>Example:</h4>

<p>Here's an example of using rayon for data parallelism:</p>

<pre><code class="language-rust line-numbers">
use rayon::prelude::*;

fn main() {
    let arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
    arr.par_iter().for_each(|&x| {
        println!("Value: {}", x);
    });
}</code></pre>

<h4>Notes:</h4>

<p>In this example, par_iter() splits the array into pieces and processes each piece concurrently using all available cores.</p>

<p>Overall, parallel computing is a powerful technique to improve the performance of algorithms and Rust provides several libraries and constructs to implement it. It's important to note that parallel algorithms can be more difficult to develop and debug compared to sequential algorithms, so it's essential to thoroughly test and use the appropriate design pattern for the problem at hand.</p>

<h3>Producer-Consumer</h3>

<p>The producer-consumer design pattern is a common approach used in concurrent programming to handle multiple threads that share a single resource. In this pattern, one thread (the producer) produces data that will be consumed by another thread (the consumer). Rust provides several tools and idioms for implementing this pattern efficiently and safely.</p>

<h4>Overview</h4>

<p>The producer-consumer pattern can be used in many scenarios, such as a database system, network servers, or message brokers. In this pattern, the producer thread generates data and places it in a shared data structure, while one or more consumer threads take the data and process it. The shared data structure, usually a queue or a buffer, acts as a buffer between the producer and the consumer threads.</p>

<p>One common problem that arises when implementing this pattern is concurrency safety - ensuring that the shared data structure is accessed safely by both the producer and consumer threads without introducing race conditions, deadlocks, or other threading issues. In Rust, several concurrency primitives and libraries can be used to help solve these problems.</p>

<h4>Implementation</h4>

<p>The code for a basic producer-consumer pattern in Rust is straightforward. The producer thread generates data and adds it to a queue, while the consumer thread takes the data from the queue and processes it:</p>

<h4>Example:</h4>

<pre><code class="language-rust line-numbers">
use std::sync::mpsc::{channel, Sender, Receiver};
use std::thread;

fn main() {
    // Create a channel for communication
    let (sender, receiver): (Sender<i32>, Receiver<i32>) = channel();

    // Start the producer thread
    let producer = thread::spawn(move || {
        for i in 0..10 {
            sender.send(i).unwrap();
        }
    });

    // Start the consumer thread
    let consumer = thread::spawn(move || {
        for _ in 0..10 {
            let data = receiver.recv().unwrap();
            println!("{}", data);
            // Process the data
        }
    });

    // Wait for both threads to finish
    producer.join().unwrap();
    consumer.join().unwrap();
}
</code></pre>

<h4>Notes</h4>

<p>In this example, we create a channel for communication between the producer and consumer threads, using the mpsc (multi-producer, single-consumer) type. The producer thread generates data, using a for loop, and sends it to the channel using the send method. The consumer thread retrieves the data from the channel, using the recv method, and processes it. Finally, the threads are joined to wait for their completion.</p>

<h3>Concurrency Safety</h3>

<p>Concurrency safety is essential when implementing a producer-consumer design pattern. Rust provides several concurrency primitives, such as locks and semaphores, that ensure concurrency safety.</p>

<p>One of the most common problems is race conditions, where two threads try to access and modify the same piece of shared data at the same time, leading to unpredictable behavior. Mutexes can be used to prevent race conditions by only allowing a single thread to access the shared data at any time.</p>

<h4>Eample</h4>

<p>Here is an example:</p>

<pre><code class="language-rust line-numbers">
use std::sync::{Arc, Mutex};
use std::thread;

fn main() {
    // Create a shared data structure using a mutex
    let data = Arc::new(Mutex::new(vec![]));

    // Clone the mutex to share across threads
    let mutex = data.clone();

    // Start the producer thread
    let producer = thread::spawn(move || {
        for i in 0..10 {
            let mut data = mutex.lock().unwrap();
            data.push(i);
        }
    });

    // Clone the mutex to share across threads
    let mutex = data.clone();

    // Start the consumer thread
    let consumer = thread::spawn(move || {
        for _ in 0..10 {
            let mut data = mutex.lock().unwrap();
            let item = data.pop().unwrap();
            println!("{}", item);
            // Process the item
        }
    });

    // Wait for both threads to finish
    producer.join().unwrap();
    consumer.join().unwrap();
}
</code></pre>

<h4>Notes:</h4>

<p>In this example, we use an Arc (atomic reference count) to share the data structure between the producer and consumer threads, and a Mutex to synchronize access to it. When the producer thread generates data, it first acquires a lock on the mutex with the lock method, modifies the data, and then releases the lock. The consumer thread follows the same procedure, but in reverse. The lock method returns a guard, which ensures that only one thread has access to the data at any time.</p>

<p>Rust also provides several other concurrency primitives for more advanced scenarios, such as RwLock, Semaphores, and Channels, all of which can be used to implement a producer-consumer pattern safely and efficiently.</p>

<div class="alert alert-info">Producer-consumer design pattern is a common approach used in concurrent programming to handle multiple threads that share a single resource. Rust provides several concurrency primitives and libraries to implement this pattern efficiently and safely, such as locks, semaphores, and channels. By following best practices and ensuring concurrency safety, developers can write efficient and high-performance multi-threading applications in Rust.</div>

<h2>Disadvantages &amp; Limitations</h2>

<p>Parallel computing can greatly improve the performance of certain computations, but it also has some disadvantages and limitations. Here are some of the common ones in the context of Rust:</p>

<ul>
  <li>Increased Complexity: Parallel computing can greatly increase the complexity of the code. It requires careful management of resources such as threads, synchronization primitives, and memory allocation, which can lead to more bugs and harder debugging. Additionally, parallel code is often more difficult to debug and maintain due to the non-deterministic nature of concurrency.</li>
  <li>Synchronization Overhead: When multiple threads access shared resources, they need to be synchronized to prevent race conditions and other concurrency-related bugs. This synchronization overhead can add significant overhead to the parallel portion of the code, reducing the potential gains from parallelization.</li>
  <li>Memory Management: Parallel computing requires careful management of memory allocation and deallocation. Allocations need to be carefully coordinated between threads to prevent issues such as data races or deadlocks.</li>
  <li>Limited Scalability: Parallel computing does not guarantee linear performance gains with the addition of more cores or threads. In some cases, adding more concurrency can actually decrease performance due to increased overheads or synchronization costs. Furthermore, some algorithms may be inherently difficult to parallelize due to dependencies or other constraints.</li>
  <li>Limited Hardware Access: Not all hardware is equally suitable for parallel computation. Some architectures, such as distributed clusters or GPUs, require specialized libraries or frameworks to efficiently utilize their resources.</li>
</ul>

<p>Overall, while parallel computing can be a powerful and efficient technique for some computations, it also requires careful attention to design and implementation to avoid the potential pitfalls and limitations of concurrency.</p>

<h2>Non-blocking processes</h2>

<p>Non-blocking processes, also known as asynchronous programming or non-blocking I/O, are a programming model where the execution of an application continues without waiting for a task to be completed before moving to the next task. In Rust, non-blocking processes are a type of concurrency that allows an application to continue its execution without blocking on I/O operations, such as reading or writing to a network socket.</p>

<p>In Rust, non-blocking I/O can be achieved using the async/await syntax, as well as the Tokio library, which is a popular Rust library for building asynchronous applications. The Tokio library provides a set of building blocks that help developers write non-blocking I/O applications efficiently.</p>

<h4>Examples:</h4>

<p>Here is an example of using async/await in Rust to perform non-blocking I/O:</p>

<pre><code class="language-rust line-numbers">
use tokio::io::AsyncReadExt;
use tokio::net::TcpStream;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut stream = TcpStream::connect("127.0.0.1:8080").await?;
    let mut buf = [0; 1024];

    // Non-blocking read from a network socket
    loop {
        let n = match stream.read(&mut buf).await {
            Ok(n) if n == 0 => break,
            Ok(n) => n,
            Err(e) => {
                println!("failed to read from socket; error = {:?}", e);
                break;
            }
        };

        println!("Received {} bytes from the server", n);
        // Process the received data
    }

    Ok(())
}
</code></pre>

<h4>Notes:</h4>

<p>In this example, we are using the tokio::io::AsyncReadExt trait to perform non-blocking I/O on a network socket. The async keyword is used to indicate that the function is asynchronous and can be executed concurrently with other tasks.</p>

<h2>Conclusion</h2>

<p>Both asynchronous execution and parallelism provide advantages in modern programming environments that utilize multi-core processors. Asynchronous execution allows for efficient use of CPU resources by allowing the overlap of computation and I/O operations, reducing the amount of idle time. This can result in faster completion times for tasks that have high I/O wait times, such as network requests or disk operations.</p>

<p>Parallelism, on the other hand, allows for the speedup of computation by breaking a task down into smaller components that can be computed simultaneously on different processor cores. This can lead to significant performance improvements on multi-core machines, as each core can work on different parts of the same task, effectively increasing processing power.</p>

<p>The choice between the two will depend on the nature of the problem, the resources available, and the specific requirements of the application. In many cases, a combination of both techniques may be used to achieve optimal performance.</p>

<hr>
<p><b>Try next:</b>
<a href="index.html#quiz">Rust Quiz</a></p>

<!-- Footer -->
<footer class="footer">
  <div class="footer-copyright text-center"></div>
</footer>
</div>
</body>
</html>